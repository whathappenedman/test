get the other alpaca models at least the higher two.
look into what else? try to get original ones like dalai x some bs
runs on cpu super slow. 
can probably get 4b version to work on xps
but most likely will work on 1660

https://www.reddit.com/r/LocalLLaMA/comments/11zcqj2/llama_optimized_for_amd_gpus/

whisper (no windows amd) (shoul be plenty fast on 1660 cus no amd gpu on windows)
and this is what ineed only tbh if needed can also run on xps easy

automatic1111 and sd (no good windows amd) 
linux rocm testing not shown on toms
1660 is slower tho even then my amd card on windows prolly (2min perimage)
no this is false its gonna be closer to a 3050 cus my 1050 is 1min30 
*(https://www.reddit.com/r/StableDiffusion/comments/x7vbt8/installed_sd_on_my_pc_running_on_a_gtx_1660ti/)
linux amd rocm should be decent but i cant :(

lets see if i can get lamma working? should work on 1660
set up chat gpt and copilot using api on laptop or some oneline thing.
- nod.ai

also set up sam from fb
# oogabooga and chat
- https://github.com/oobabooga/text-generation-webui

DIDNT TRY laptop 1050 might work for whisper and stable
idk about llama
but try that. otherwise short term get 3060 cus has alot of vram
other wise xx90 cards all the way 
used 3090s non ti shoulkd be good. two for nvlink and ur set
l4 are replacing t4 this is low power passive
otherwise the a data center ones 



for my ai
[keep notes]


# to dos

get ai up and running

open ai whisper
or 
https://github.com/facebookresearch/fairseq/blob/main/examples/wav2vec/README.md

stable diffusion!


voice recog good
text to speech good

ocr good
https://github.com/tesseract-ocr/tesseract#installing-tesseract
https://news.ycombinator.com/item?id=27876383
first comment on wolf binary
https://hn.algolia.com/?q=ocr

works good and fast, have to resise image like 5mb image to 50%

obj detect ok
image class ok
both by google not impo rn

summary bootleg
trans
snetiment bootleg (make an extention to see what it says for paragraphs lol)

text,image,video gen


## SUammary

https://huggingface.co/facebook/bart-large-cnn?text=Coupled+with+Nvidia%E2%80%99s+latest+update+to+its+Deep+Learning+Super+Sampling+%28DLSS%29%2C+the+RTX+4090+can+deliver+the+type+of+smooth+4K+performance+in+Cyberpunk+2077+that%E2%80%99s+never+been+possible+before.+I%E2%80%99m+talking+nearly+140fps+at+4K+with+all+settings+maxed+out+and+psycho+ray+tracing+enabled.+And+at+1440p%2C+it%E2%80%99s+nearly+hitting+the+limit+of+my+240Hz+monitor.%0A%0ADLSS+3+provides+an+incredibly+impressive+performance+bump%2C+but+in+games+where+it%E2%80%99s+not+yet+supported%2C+you%E2%80%99re+still+getting+around+70+percent+better+frame+rates+on+average.%0A%0AAll+of+this+comes+at+a+cost%2C+though.+At+%241%2C599%2C+the+RTX+4090+is+at+the+very+top+end+of+the+GPU+market%2C+and+its+huge+footprint+could+make+it+difficult+to+slot+into+some+PC+cases.+The+RTX+4090+also+uses+450+watts+of+power%2C+and+you%E2%80%99ll+need+plenty+of+spare+PCIe+power+connectors+to+get+this+card+powered+up.+But+if+you+can+deal+with+the+price%2C+size%2C+and+power+requirements%2C+this+is+a+card+capable+of+delivering+a+4K+experience+like+no+other.%0A%0AThe+RTX+4090+Founders+Edition+ships+in+a+box+that%E2%80%99s+comically+huge.+It%E2%80%99s+how+I+imagine+you%E2%80%99d+be+sent+a+winning+check+from+the+lottery+because+the+oversize+box+really+prepares+you+for+the+surreal+size+of+the+RTX+4090.%0A%0AAlthough+it%E2%80%99s+nearly+10mm+shorter+than+the+RTX+3090%2C+the+extra+inch+in+height+makes+it+look+a+lot+chunkier.+I+didn%E2%80%99t+have+any+problems+fitting+it+into+my+case%2C+but+I+can+imagine+the+sheer+size+will+be+problematic+in+some+cases%2C+particularly+if+you+pick+a+third-party+RTX+4090+where+the+sizes+go+way+beyond+the+Founders+Edition+models.%0A%0Avidia+has+also+switched+up+its+power+connector+with+the+RTX+4090%2C+and+it%E2%80%99s+using+the+single+12-pin+PCIe+5+standard+that+can+deliver+up+to+600+watts+in+total.+It%E2%80%99s+the+same+connector+found+on+the+RTX+3090+Ti%2C+and+Nvidia+ships+a+12VHPWR+adapter+cable+in+the+box+so+you+can+connect+four+eight-pin+PCIe+power+cables.+That%E2%80%99s+double+the+amount+of+eight-pin+cables+needed+over+the+RTX+3090%2C+but+you+technically+only+need+to+connect+three+eight-pin+cables+that+can+deliver+150+watts+each.+That+covers+450+watts+in+total%2C+and+Nvidia+recommends+connecting+the+fourth+eight-pin+if+you%E2%80%99re+planning+to+overclock+the+card.%0A%0AYou%E2%80%99ll+probably+want+to+have+a+1%2C000-watt+power+supply+on+hand+for+the+RTX+4090%2C+particularly+because+you%E2%80%99ll+need+to+pair+this+with+the+latest+generation+CPUs+to+really+take+advantage+of+the+power+on+offer+here.+Nvidia+recommends+850+watts+in+total%2C+but+having+that+extra+headroom+for+overclocking+could+be+useful.%0A%0AI%E2%80%99m+still+a+big+fan+of+the+Founders+Edition+design+for+Nvidia%E2%80%99s+RTX+cards%2C+even+if+very+little+has+changed+on+the+outside+for+this+Ada+Lovelace+generation.

take verge article on 4090
it works good enough.
have my personal guys fix it and keep retraining?

https://huggingface.co/google/pegasus-cnn_dailymail?text=Coupled+with+Nvidia%E2%80%99s+latest+update+to+its+Deep+Learning+Super+Sampling+%28DLSS%29%2C+the+RTX+4090+can+deliver+the+type+of+smooth+4K+performance+in+Cyberpunk+2077+that%E2%80%99s+never+been+possible+before.+I%E2%80%99m+talking+nearly+140fps+at+4K+with+all+settings+maxed+out+and+psycho+ray+tracing+enabled.+And+at+1440p%2C+it%E2%80%99s+nearly+hitting+the+limit+of+my+240Hz+monitor.%0A%0ADLSS+3+provides+an+incredibly+impressive+performance+bump%2C+but+in+games+where+it%E2%80%99s+not+yet+supported%2C+you%E2%80%99re+still+getting+around+70+percent+better+frame+rates+on+average.%0A%0AAll+of+this+comes+at+a+cost%2C+though.+At+%241%2C599%2C+the+RTX+4090+is+at+the+very+top+end+of+the+GPU+market%2C+and+its+huge+footprint+could+make+it+difficult+to+slot+into+some+PC+cases.+The+RTX+4090+also+uses+450+watts+of+power%2C+and+you%E2%80%99ll+need+plenty+of+spare+PCIe+power+connectors+to+get+this+card+powered+up.+But+if+you+can+deal+with+the+price%2C+size%2C+and+power+requirements%2C+this+is+a+card+capable+of+delivering+a+4K+experience+like+no+other.%0A%0AThe+RTX+4090+Founders+Edition+ships+in+a+box+that%E2%80%99s+comically+huge.+It%E2%80%99s+how+I+imagine+you%E2%80%99d+be+sent+a+winning+check+from+the+lottery+because+the+oversize+box+really+prepares+you+for+the+surreal+size+of+the+RTX+4090.%0A%0AAlthough+it%E2%80%99s+nearly+10mm+shorter+than+the+RTX+3090%2C+the+extra+inch+in+height+makes+it+look+a+lot+chunkier.+I+didn%E2%80%99t+have+any+problems+fitting+it+into+my+case%2C+but+I+can+imagine+the+sheer+size+will+be+problematic+in+some+cases%2C+particularly+if+you+pick+a+third-party+RTX+4090+where+the+sizes+go+way+beyond+the+Founders+Edition+models.%0A%0Avidia+has+also+switched+up+its+power+connector+with+the+RTX+4090%2C+and+it%E2%80%99s+using+the+single+12-pin+PCIe+5+standard+that+can+deliver+up+to+600+watts+in+total.+It%E2%80%99s+the+same+connector+found+on+the+RTX+3090+Ti%2C+and+Nvidia+ships+a+12VHPWR+adapter+cable+in+the+box+so+you+can+connect+four+eight-pin+PCIe+power+cables.+That%E2%80%99s+double+the+amount+of+eight-pin+cables+needed+over+the+RTX+3090%2C+but+you+technically+only+need+to+connect+three+eight-pin+cables+that+can+deliver+150+watts+each.+That+covers+450+watts+in+total%2C+and+Nvidia+recommends+connecting+the+fourth+eight-pin+if+you%E2%80%99re+planning+to+overclock+the+card.%0A%0AYou%E2%80%99ll+probably+want+to+have+a+1%2C000-watt+power+supply+on+hand+for+the+RTX+4090%2C+particularly+because+you%E2%80%99ll+need+to+pair+this+with+the+latest+generation+CPUs+to+really+take+advantage+of+the+power+on+offer+here.+Nvidia+recommends+850+watts+in+total%2C+but+having+that+extra+headroom+for+overclocking+could+be+useful.%0A%0AI%E2%80%99m+still+a+big+fan+of+the+Founders+Edition+design+for+Nvidia%E2%80%99s+RTX+cards%2C+even+if+very+little+has+changed+on+the+outside+for+this+Ada+Lovelace+generation.

this on was better so like run it through a few to help the team.





# buying
want tensro cores
want nvidia!


want 24gb other wise 12 is good 16 is meh

20 30 or 40 series has tensor cores
v100,t4 or A

might as well just focus on 30 series

can get a 3060 12gb used for under 200ia
hows that compare to 1080 11gb? no tensorcores tho
toms sd:
timdettemers:


yea so 3060 12 gb is no brainer
over 1080 its no tensor is less .75 : 1
same ram thho
less power
and TENSOR CORES!


for higher price its
3090 24 vs 4070ti
4070ti come out a bit ahead esp tensor on 8bit but less ram but less pwoer

so amd @100 not worth
1080ti @160 is so close to 306012gb @200ish
can also do 2060 SUPER and higher at this price but its less ram (8v12)

at higher it  gets messy

2080ti 11gb @350

or 3080 (12) and up
or 4070(12) and up or

titan rtx (20 series) 24gb
3090 24gb


https://www.tomshardware.com/news/stable-diffusion-gpu-benchmarks
https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/#Raw_Performance_Ranking_of_GPUs



# whisper
works good both on laptop gpu and on desktop cpu(cpp). can probably get it working on iphone too.
need to get speakers, and smooth api flow with ffmpeg and docker?

# use openai api till you figure out llama and stable lm
lama.cpp is not as good as whisper cpp was.
try to cuda llama and run on 6gb

# NB
llm prolly wont work
wshiper and sam will work on all three platforms
so basically just talking about sd
eaither way get nas set up first.
